# Training Curriculum & Data Strategy

This document outlines the architecture for training EpiForecaster using a **"Teacher-Student"** curriculum that bridges **Synthetic Data** (generated by physical models) and **Real Data** (sparse, noisy observations).

## Core Philosophy: The Annealed Mixture

To avoid "catastrophic forgetting" of the disease dynamics, we use a **gradually shifting mixture** of data streams.

### The Phases

0.  **Phase 0: Synthetic Pretraining (Synthetic Only)**
    *   **Goal:** Learn broad disease dynamics across many synthetic runs before mixing.
    *   **Sampling:** 100% Synthetic.
    *   **Mode:** Determined by dataset ordering (`data.sample_ordering`).
    *   **Notes:** Each epoch can correspond to a *new* synthetic run, so we can
        run many fully synthetic epochs before mixing in real data.

1.  **Phase 1: Physics Pre-training (Synthetic Heavy)**
    *   **Goal:** Teach the model fundamental disease dynamics and causal effects of interventions.
    *   **Data Source:** Synthetic Generator (Twin Scenarios).
    *   **Sampling:** 80% Synthetic / 20% Real.
    *   **Mode:** Determined by dataset ordering (`data.sample_ordering`).
    *   **Complexity:** Start with clean synthetic data, ramping up to noisy (`missing_rate=0.05`).

2.  **Phase 2: Domain Adaptation (Balanced)**
    *   **Goal:** Adapt to real-world artifacts (delays, noise) while maintaining causal reasoning.
    *   **Sampling:** 50% Synthetic / 50% Real.
    *   **Mode:** Determined by dataset ordering (`data.sample_ordering`).
    *   **Synthetic Sampling:** Prioritize "Twin Scenarios" with strong interventions to reinforce the signal of mobility reduction.

3.  **Phase 3: Specialist Fine-tuning (Real Heavy)**
    *   **Goal:** Maximize performance on the specific target distribution (e.g., Catalonia).
    *   **Sampling:** 5% Synthetic (Regularizer) / 95% Real.
    *   **Mode:** Determined by dataset ordering (`data.sample_ordering`).

---

## Architecture: The "Director" (Curriculum Sampler)

We move decision logic out of the `Dataset` and into a stateful **`BatchSampler`**.

### `EpidemicCurriculumSampler`

```python
@dataclass
class CurriculumState:
    epoch: int = 0
    synth_ratio: float = 0.8
    mode: str = "time_major"  # 'time_major' vs 'node_major'
    focus_lockdowns: bool = False

class EpidemicCurriculumSampler(BatchSampler):
    def __init__(self, dataset, batch_size):
        # Build O(1) maps: WindowID -> [Indices], NodeID -> [Indices]
        self.time_to_indices = ... 
        self.node_to_indices = ...
        self.synth_indices = ...
        self.real_indices = ...

    def set_curriculum(self, epoch):
        # Update state based on epoch schedule
        if epoch < 10:
            self.state.synth_ratio = 0.8
        elif epoch < 50:
            self.state.synth_ratio = 0.2
        else:
            self.state.synth_ratio = 0.05

    def __iter__(self):
        # Dataset ordering (time-major vs node-major) is handled at dataset creation.
        # The sampler mixes real/synthetic batches without reordering samples.
        ...
```

### Dataset Contract for Curriculum Mixing (Per-Run Datasets)

We will keep **one Zarr store** that contains all runs, but initialize
**one dataset per run** by filtering on `run_id` during dataset construction.
The training set is a `ConcatDataset` over those per-run datasets.

*   **run_id coordinate/dimension:** Present in the Zarr for selection.
    *   Real data labeled `"real"` (single run).
    *   Synthetic runs labeled with descriptive strings (e.g., `"synth_run_001"`).
*   **Per-run datasets:** `EpiDataset(..., run_id="synth_run_001")` loads a
    single run and behaves like the current single-run pipeline (3D tensors).
*   **Mobility Format:** Both real and synthetic data use **dense** `mobility`.
    *   Synthetic data is **unfactorized** during preprocessing to match real.
*   **Sampler Split:** `synth_datasets` and `real_dataset` are grouped at the
    dataset level. The sampler chooses a dataset (run) first, then samples
    windows/nodes from that dataset.
*   **Memory behavior:** We only keep **one synthetic run + one real dataset**
    active per epoch to limit memory and preserve locality.

This preserves per-run validity/missingness logic while enabling curriculum
mixing without forcing a 4D run dimension through the dataset pipeline.

### Chunked Interleaving Plan (Balanced Mixing + Locality)

To balance synthetic/real mixing while preserving predictable mobility access
patterns, the sampler uses **chunked interleaving**:

1. **Precompute window pools**
   * Build `dataset_id -> [window_indices]` for synthetic run datasets.
   * Build `real_indices` for the real dataset.

2. **Select active synthetic run**
   * Each epoch corresponds to **exactly one synthetic run** (round-robin or random).
   * This lets us generate many synthetic runs and avoid overfitting any one run.

3. **Yield in chunks**
   * For each active run, take a **contiguous chunk** of `chunk_size` windows
     (e.g., 256â€“1024 windows).
   * Emit `k` synthetic batches from the chunk and interleave `m` real batches
     to match `synth_ratio`.

4. **Rotate**
   * Move to the next chunk within the same run dataset until exhausted, then
     rotate to the next run dataset on the next epoch.

This keeps time-contiguous access for synthetic mobility slices while still
achieving a balanced mixture at the batch level.

### Config Expectations (training.curriculum)

Curriculum behavior is controlled under `training.curriculum` in the training
config. When curriculum is disabled, training should fall back to a single
run slice using `training.run_id` (default: `"real"`).

The curriculum schedule is an **ordered array** of phase definitions. All
phase boundaries and sampling ratios are intended to be tuned (e.g., via
Optuna), so keep them configurable in YAML.

```yaml
training:
  run_id: "real"  # used when curriculum.enabled = false
  curriculum:
    enabled: true
    chunk_size: 512         # contiguous windows per run
    run_sampling: "round_robin"  # or "random"
    schedule:
      - start_epoch: 0
        end_epoch: 5
        synth_ratio: 1.0
        mode: "time_major"  # kept constant; dataset ordering determines mode
      - start_epoch: 5
        end_epoch: 10
        synth_ratio: 0.8
        mode: "time_major"
      - start_epoch: 10
        end_epoch: 50
        synth_ratio: 0.5
        mode: "time_major"
      - start_epoch: 50
        end_epoch: 999
        synth_ratio: 0.05
        mode: "time_major"
```

---

## Implementation: The "Assembler" (Collate Function)

Since we use complex temporal graphs, we manually batch the PyG objects.

### Standard Collate (Current)

*   **Input:** List of `EpiDatasetItem` (each containing a list of $L$ graphs).
*   **Output:** `MobBatch` is a single flattened `Batch` with `B` and `T` stored
    on the batch for reshaping in the model.

This keeps the model interface unchanged even when curriculum is enabled.
Batch ordering handles real/synthetic mixing; we do not mix within a batch.

### Imported Risk Note (Dense Mobility Only)

Imported risk features currently require **dense** mobility to precompute
time-varying lagged risk. This is compatible with the single-dataset approach
above because synthetic mobility is unfactorized. If factorized mobility is
reintroduced later, imported risk should be disabled or re-implemented with
streaming reconstruction.

---

## Data Engineering Requirements

### 1. Synthetic Data Alignment (Feedback to Generator)

To ensure the synthetic data is indistinguishable from real data to the preprocessing pipeline:

*   **Explicit LoD:** The generator **must** output a `detection_limit` (or `LD(CG/L)`) variable for every wastewater measurement.
    *   *Why:* The `EDARProcessor` uses `limit_flow = detection_limit * flow_rate` to set the censoring floor for the Tobit Kalman Filter. Without this, synthetic censoring is handled incorrectly.
*   **Variable Naming:** Ensure `cases`, `mobility`, `population` match the real Zarr structure.

### 2. Mobility Format (Unified Dense)

For curriculum training we use **dense mobility** for both real and synthetic
data to keep a single dataset format.

*   **Synthetic preprocessing:** Unfactorize mobility to dense `mobility`.
*   **Real preprocessing:** Add `run_id` as a dimension with value `"real"`.
*   **Synthetic preprocessing:** Add descriptive `run_id` strings for each run
    (e.g., `"synth_run_001"`).

---

## Next Steps

1.  **Update Synthetic Generator:** Add `detection_limit` output.
2.  **Update Preprocessing:**
    *   Unfactorize synthetic mobility to dense `mobility`.
    *   Add `run_id` dimension with strings for synthetic runs.
    *   Add `run_id="real"` for real data.
3.  **Implement Sampler:** Create `EpidemicCurriculumSampler` in `data/samplers.py`.
4.  **Update Trainer:** Integrate the sampler into `epiforecaster_trainer.py`.

"""
Processor for synthetic epidemiological data from zarr bundles.

This module handles loading synthetic data that was generated by EpiSim.jl
and processed through the synthetic generator pipeline. The synthetic data
is bundled in a single zarr file with all variables.

Synthetic data MUST use concentration mode (not total_flow) because it
contains pre-computed concentrations without flow rate information.
"""

import numpy as np
import xarray as xr

from ..config import REGION_COORD, TEMPORAL_COORD, PreprocessingConfig


class SyntheticProcessor:
    """
    Loads and extracts synthetic data from bundled zarr format.

    This processor handles:
    - Loading synthetic data from a single zarr bundle
    - Validating flow_mode is concentration (synthetic has no flow_rate)
    - Extracting individual data sources (cases, mobility, EDAR, population)
    - Preserving the factorized mobility format for the MobilityProcessor

    The output is a dict of xarray objects compatible with the alignment processor.
    """

    def __init__(self, config: PreprocessingConfig):
        """
        Initialize the synthetic processor.

        Args:
            config: Preprocessing configuration

        Raises:
            ValueError: If wastewater_flow_mode is not "concentration"
        """
        self.config = config

        # Synthetic data contains concentrations, not flow
        if config.wastewater_flow_mode != "concentration":
            raise ValueError(
                f"Synthetic data requires wastewater_flow_mode='concentration', "
                f"got '{config.wastewater_flow_mode}'. "
                f"Synthetic data has no flow_rate variable."
            )

    def process(
        self, synthetic_path: str, run_filter: list[int] | None = None
    ) -> dict[str, xr.DataArray | xr.Dataset]:
        """
        Load and extract synthetic data from bundled zarr.

        Args:
            synthetic_path: Path to raw_synthetic_observations.zarr
            run_filter: Optional list of run_ids to include. If None, includes all runs.

        Returns:
            Dict with keys: cases, mobility, edar, population
        """
        print(f"Loading synthetic data from {synthetic_path}")

        ds = xr.open_zarr(synthetic_path)
        print(ds)
        print()

        # Filter runs if specified
        if run_filter is not None:
            run_ids = ds["run_id"].values
            mask = np.isin(run_ids, run_filter)
            ds = ds.isel(run_id=mask)
            print(f"Filtered to {len(ds.run_id)} runs: {ds.run_id.values}")
            print()

        # Validate required variables exist
        required_vars = ["cases", "mobility_base", "mobility_kappa0", "population"]
        missing = [v for v in required_vars if v not in ds]
        if missing:
            raise ValueError(f"Synthetic data missing required variables: {missing}")

        # Check for EDAR biomarkers (optional but recommended)
        # type: ignore[reportOperatorIssue] (xarray data_vars iteration)
        edar_vars = [
            v for v in ds.data_vars if "edar_biomarker" in str(v) and "_LoD" in str(v)
        ]
        if not edar_vars:
            print(
                "Warning: No EDAR LoD variables found. Wastewater preprocessing will fail."
            )

        # EDAR returns tuple (flow_xr, censor_xr) for shared processing path
        edar_flow, edar_censor = self._extract_edar(ds)

        result = {
            "cases": self._extract_cases(ds),
            "mobility": self._extract_mobility(ds),
            "edar_flow": edar_flow,  # Flow/concentration values
            "edar_censor": edar_censor,  # Censor flags
            "population": self._extract_population(ds),
        }

        return result

    def _extract_cases(self, ds: xr.Dataset) -> xr.Dataset:
        """Extract cases data from synthetic bundle.

        Expected shape: (run_id, date, region_id)
        We need to stack all runs into the temporal dimension.
        """
        print("Extracting cases...")

        # Cases: (run_id, date, region_id) → restructure to match expected format
        cases = ds["cases"]  # (run_id, date, region_id)

        # For synthetic data with multiple runs, we have a few options:
        # 1. Stack runs into the temporal dimension (concatenate time series)
        # 2. Select a single run
        # 3. Process each run separately

        # For now, let's use the first run (typically Baseline) as the canonical dataset
        # TODO: Add support for multi-run training
        cases_0 = cases.isel(run_id=0)

        # Verify temporal dimension matches config
        cases_0 = cases_0.rename({TEMPORAL_COORD: TEMPORAL_COORD})

        # Crop to config date range
        start_date = np.datetime64(self.config.start_date)
        end_date = np.datetime64(self.config.end_date)

        time_mask = (cases_0[TEMPORAL_COORD] >= start_date) & (
            cases_0[TEMPORAL_COORD] <= end_date
        )
        cases_filtered = cases_0.isel({TEMPORAL_COORD: time_mask})

        assert not cases_filtered.sizes[TEMPORAL_COORD] == 0, (
            "No cases data in temporal range"
        )

        print(f"  ✓ Extracted cases: {cases_filtered.shape}")
        return cases_filtered.to_dataset(name="cases")

    def _extract_mobility(self, ds: xr.Dataset) -> xr.Dataset:
        """Extract and reconstruct mobility data from factorized format.

        Returns dataset with reconstructed mobility tensor.
        """
        print("Extracting mobility (factorized format)...")

        mobility_base = ds["mobility_base"]  # (origin, target)
        mobility_kappa0 = ds["mobility_kappa0"]  # (run_id, date)

        # Use first run's kappa0
        kappa0_0 = mobility_kappa0.isel(run_id=0)

        # Rename to match expected coords
        mobility_base = mobility_base.rename(
            {"origin": REGION_COORD, "target": "destination"}
        )
        mobility_base = mobility_base.rename({REGION_COORD: "origin"})

        # Filter by date range
        start_date = np.datetime64(self.config.start_date)
        end_date = np.datetime64(self.config.end_date)

        time_mask = (kappa0_0[TEMPORAL_COORD] >= start_date) & (
            kappa0_0[TEMPORAL_COORD] <= end_date
        )
        kappa0_filtered = kappa0_0.isel({TEMPORAL_COORD: time_mask})

        # Reconstruct mobility tensor: mobility[date, origin, destination]
        # Using formula: mobility[date] = mobility_base * (1 - kappa0[date])
        # Use xarray broadcasting to preserve dask chunking
        reduction_factor = 1.0 - kappa0_filtered  # (date,)
        # Expand dims with actual coordinate values for proper alignment
        reduction_factor_expanded = reduction_factor.expand_dims(
            {
                "origin": mobility_base["origin"].values,
                "destination": mobility_base["destination"].values,
            }
        )
        mobility_reconstructed = mobility_base * reduction_factor_expanded

        # Reorder dimensions to (date, origin, destination)
        mobility_reconstructed = mobility_reconstructed.transpose(
            TEMPORAL_COORD, "origin", "destination"
        )

        mobility_da = xr.DataArray(
            mobility_reconstructed,
            dims=(TEMPORAL_COORD, "origin", "destination"),
        )

        mobility_ds = mobility_da.to_dataset(name="mobility")

        print(f"  ✓ Reconstructed mobility tensor: {mobility_da.shape}")
        return mobility_ds

    def _extract_edar(self, ds: xr.Dataset) -> tuple[xr.DataArray, xr.DataArray]:
        """
        Extract EDAR biomarker data in the same intermediate format as real data.

        Expected variables:
        - edar_biomarker_N1, N2, IP4: (run_id, date, edar_id)
        - edar_biomarker_N1_LoD, N2_LoD, IP4_LoD: (run_id, edar_id)

        Returns:
            Tuple of (flow_xr, censor_xr) with dimensions (date, edar_id, variant)
            Compatible with EDARProcessor.process_from_xarray()
        """
        print("Extracting EDAR biomarkers...")

        from constants import EDAR_BIOMARKER_PREFIX, EDAR_BIOMARKER_VARIANTS

        run_idx = 0  # Use first run (typically Baseline)
        flow_list = []
        censor_list = []

        for variant in EDAR_BIOMARKER_VARIANTS:
            var_name = f"{EDAR_BIOMARKER_PREFIX}{variant}"
            lod_name = f"{var_name}_LoD"

            if var_name not in ds:
                continue
            if lod_name not in ds:
                print(f"  Warning: {lod_name} not found, skipping {variant}")
                continue

            # Extract biomarker values: (date, edar_id)
            biomarker = ds[var_name].isel(run_id=run_idx)
            lod = ds[lod_name].isel(run_id=run_idx)  # (edar_id,)

            # Filter by date range
            start_date = np.datetime64(self.config.start_date)
            end_date = np.datetime64(self.config.end_date)

            time_mask = (biomarker[TEMPORAL_COORD] >= start_date) & (
                biomarker[TEMPORAL_COORD] <= end_date
            )
            biomarker_filtered = biomarker.isel({TEMPORAL_COORD: time_mask})

            # Compute censor flag: 1.0 if value <= LoD, else 0.0 (for finite values only)
            censor = xr.where(
                (biomarker_filtered <= lod) & biomarker_filtered.notnull(), 1.0, 0.0
            ).fillna(0.0)

            flow_list.append(biomarker_filtered)
            censor_list.append(censor)

        if not flow_list:
            raise ValueError("No EDAR biomarker data found in synthetic dataset")

        # Stack into (date, edar_id, variant) format - same as real data!
        flow_xr = xr.concat(flow_list, dim="variant").assign_coords(
            variant=EDAR_BIOMARKER_VARIANTS[: len(flow_list)]
        )
        censor_xr = xr.concat(censor_list, dim="variant").assign_coords(
            variant=EDAR_BIOMARKER_VARIANTS[: len(censor_list)]
        )

        print(f"  ✓ Extracted {len(flow_list)} biomarker variants")
        return flow_xr, censor_xr

    def _extract_population(self, ds: xr.Dataset) -> xr.DataArray:
        """Extract population data.

        Expected: (run_id, region_id) - use first run.
        """
        print("Extracting population...")

        # Use first run
        pop = ds["population"].isel(run_id=0)

        # Verify region coord matches
        pop = pop.rename({"region_id": REGION_COORD})

        print(f"  ✓ Extracted population: {pop.shape}")
        return pop

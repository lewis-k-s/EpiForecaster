"""
Processor for synthetic epidemiological data from zarr bundles.

This module handles loading synthetic data that was generated by EpiSim.jl
and processed through the synthetic generator pipeline. The synthetic data
is bundled in a single zarr file with all variables.

Synthetic data MUST use concentration mode (not total_flow) because it
contains pre-computed concentrations without flow rate information.
"""

import numpy as np
import xarray as xr

from ..config import REGION_COORD, TEMPORAL_COORD, PreprocessingConfig


class SyntheticProcessor:
    """
    Loads and extracts synthetic data from bundled zarr format.

    This processor handles:
    - Loading synthetic data from a single zarr bundle
    - Validating flow_mode is concentration (synthetic has no flow_rate)
    - Extracting individual data sources (cases, mobility, EDAR, population)
    - Preserving the factorized mobility format for the MobilityProcessor

    The output is a dict of xarray objects compatible with the alignment processor.
    """

    def __init__(self, config: PreprocessingConfig):
        """
        Initialize the synthetic processor.

        Args:
            config: Preprocessing configuration

        Raises:
            ValueError: If wastewater_flow_mode is not "concentration"
        """
        self.config = config

        # Synthetic data contains concentrations, not flow
        if config.wastewater_flow_mode != "concentration":
            raise ValueError(
                f"Synthetic data requires wastewater_flow_mode='concentration', "
                f"got '{config.wastewater_flow_mode}'. "
                f"Synthetic data has no flow_rate variable."
            )

    def process(
        self, synthetic_path: str, run_filter: list[str] | list[int] | None = None
    ) -> dict[str, xr.DataArray | xr.Dataset]:
        """
        Load and extract synthetic data from bundled zarr.

        Args:
            synthetic_path: Path to raw_synthetic_observations.zarr
            run_filter: Optional list of run_ids to include. If None, includes all runs.

        Returns:
            Dict with keys: cases, mobility, edar, population
        """
        print(f"Loading synthetic data from {synthetic_path}")

        ds = xr.open_zarr(
            synthetic_path,
            chunks={"run_id": self.config.run_id_chunk_size},  # type: ignore[arg-type]
        )
        print(ds)
        print()

        # Filter runs if specified
        if run_filter is not None:
            run_ids = ds["run_id"].values
            mask = np.isin(run_ids, run_filter)
            ds = ds.isel(run_id=mask)
            print(f"Filtered to {len(ds.run_id)} runs: {ds.run_id.values}")
            print()

        # Validate required variables exist
        # Mobility can be in two formats:
        # - New: mobility_time_varying (pre-computed with kappa0 applied)
        # - Legacy: mobility_base + mobility_kappa0 (factorized format)
        required_vars = ["cases", "population"]
        missing = [v for v in required_vars if v not in ds]
        if missing:
            raise ValueError(f"Synthetic data missing required variables: {missing}")

        # Check mobility format
        has_time_varying = "mobility_time_varying" in ds
        has_factorized = "mobility_base" in ds and "mobility_kappa0" in ds
        if not has_time_varying and not has_factorized:
            raise ValueError(
                "Synthetic data missing mobility data. "
                "Expected either 'mobility_time_varying' or "
                "('mobility_base' + 'mobility_kappa0')"
            )

        synthetic_sparsity_level = None
        if "synthetic_sparsity_level" in ds:
            synthetic_sparsity_level = ds["synthetic_sparsity_level"]
            print(
                "  ✓ Found synthetic_sparsity_level metadata: "
                f"{synthetic_sparsity_level.shape}"
            )
        else:
            print(
                "Warning: synthetic_sparsity_level not found; "
                "curriculum sparsity filtering will be unavailable."
            )

        # Check for EDAR biomarkers (optional but recommended)
        # type: ignore[reportOperatorIssue] (xarray data_vars iteration)
        edar_vars = [
            v for v in ds.data_vars if "edar_biomarker" in str(v) and "_LoD" in str(v)
        ]
        if not edar_vars:
            print(
                "Warning: No EDAR LoD variables found. Wastewater preprocessing will fail."
            )

        # EDAR returns tuple (flow_xr, censor_xr) for shared processing path
        edar_flow, edar_censor = self._extract_edar(ds)

        result = {
            "cases": self._extract_cases(ds),
            "mobility": self._extract_mobility(ds),
            "edar_flow": edar_flow,  # Flow/concentration values
            "edar_censor": edar_censor,  # Censor flags
            "population": self._extract_population(ds),
        }

        # Extract optional hospitalizations and deaths data
        hosp_data = self._extract_hospitalizations(ds)
        if hosp_data is not None:
            result["hospitalizations"] = hosp_data

        deaths_data = self._extract_deaths(ds)
        if deaths_data is not None:
            result["deaths"] = deaths_data

        if synthetic_sparsity_level is not None:
            result["synthetic_sparsity_level"] = synthetic_sparsity_level

        return result

    def _extract_cases(self, ds: xr.Dataset) -> xr.Dataset:
        """Extract cases data from synthetic bundle.

        Expected shape: (run_id, date, region_id)
        Preserves run_id dimension for curriculum training.
        """
        print("Extracting cases...")

        # Cases: (run_id, date, region_id) → preserve run_id dimension
        cases = ds["cases"]

        # Verify temporal dimension matches config
        cases = cases.rename({TEMPORAL_COORD: TEMPORAL_COORD})

        # Crop to config date range (applies to all runs)
        start_date = np.datetime64(self.config.start_date)
        end_date = np.datetime64(self.config.end_date)

        time_mask = (cases[TEMPORAL_COORD] >= start_date) & (
            cases[TEMPORAL_COORD] <= end_date
        )
        cases_filtered = cases.isel({TEMPORAL_COORD: time_mask})

        assert not cases_filtered.sizes[TEMPORAL_COORD] == 0, (
            "No cases data in temporal range"
        )

        num_runs = len(cases_filtered.run_id)
        print(f"  ✓ Extracted cases with {num_runs} runs: {cases_filtered.shape}")
        return cases_filtered.to_dataset(name="cases")

    def _extract_mobility(self, ds: xr.Dataset) -> xr.Dataset:
        """Extract mobility data from synthetic dataset.

        Supports two formats:
        1. New: mobility_time_varying (pre-computed with kappa0 applied)
        2. Legacy: mobility_base + mobility_kappa0 (factorized format)

        Preserves run_id dimension.
        Returns dataset with dimensions (run_id, date, origin, destination).
        """
        print("Extracting mobility...")

        # Check for time-varying format (new synthetic data)
        if "mobility_time_varying" in ds:
            print("  Found mobility_time_varying (pre-computed format)")
            mobility = ds["mobility_time_varying"]

            # Transpose to match expected dimension order: (run_id, date, origin, destination)
            # Input dims: (run_id, origin, target, date)
            mobility = mobility.transpose("run_id", TEMPORAL_COORD, "origin", "target")
            mobility = mobility.rename({"target": "destination"})

            # Filter by date range
            start_date = np.datetime64(self.config.start_date)
            end_date = np.datetime64(self.config.end_date)
            time_mask = (mobility[TEMPORAL_COORD] >= start_date) & (
                mobility[TEMPORAL_COORD] <= end_date
            )
            mobility_filtered = mobility.isel({TEMPORAL_COORD: time_mask})

            num_runs = len(mobility_filtered.run_id)
            print(
                f"  ✓ Extracted mobility with {num_runs} runs: {mobility_filtered.shape}"
            )
            return mobility_filtered.to_dataset(name="mobility")

        # Fall back to factorized format (legacy synthetic data)
        print("  Using factorized format (mobility_base + mobility_kappa0)")

        mobility_base = ds["mobility_base"]  # (origin, target)
        mobility_kappa0 = ds["mobility_kappa0"]  # (run_id, date)

        # Rename to match expected coords
        mobility_base = mobility_base.rename(
            {"origin": REGION_COORD, "target": "destination"}
        )
        mobility_base = mobility_base.rename({REGION_COORD: "origin"})

        # Filter by date range (applies to all runs)
        start_date = np.datetime64(self.config.start_date)
        end_date = np.datetime64(self.config.end_date)

        time_mask = (mobility_kappa0[TEMPORAL_COORD] >= start_date) & (
            mobility_kappa0[TEMPORAL_COORD] <= end_date
        )
        kappa0_filtered = mobility_kappa0.isel({TEMPORAL_COORD: time_mask})

        # Reconstruct mobility tensor: mobility[run_id, date, origin, destination]
        # Using formula: mobility[date] = mobility_base * (1 - kappa0[date])
        # Use xarray broadcasting to preserve dask chunking
        reduction_factor = 1.0 - kappa0_filtered  # (run_id, date)

        # Expand dims with actual coordinate values for proper alignment
        # reduction_factor: (run_id, date) -> (run_id, date, origin, destination)
        reduction_factor_expanded = reduction_factor.expand_dims(
            {
                "origin": mobility_base["origin"].values,
                "destination": mobility_base["destination"].values,
            }
        )

        # mobility_base: (origin, destination) -> broadcast to (run_id, date, origin, destination)
        mobility_reconstructed = mobility_base * reduction_factor_expanded

        # Reorder dimensions to (run_id, date, origin, destination)
        mobility_reconstructed = mobility_reconstructed.transpose(
            "run_id", TEMPORAL_COORD, "origin", "destination"
        )

        mobility_ds = mobility_reconstructed.to_dataset(name="mobility")

        num_runs = len(mobility_reconstructed.run_id)
        print(
            f"  ✓ Reconstructed mobility tensor with {num_runs} runs: {mobility_reconstructed.shape}"
        )
        return mobility_ds

    def _extract_edar(self, ds: xr.Dataset) -> tuple[xr.DataArray, xr.DataArray]:
        """
        Extract EDAR biomarker data in the same intermediate format as real data.

        Preserves run_id dimension.

        Expected variables:
        - edar_biomarker_N1, N2, IP4: (run_id, date, edar_id)
        - edar_biomarker_N1_LoD, N2_LoD, IP4_LoD: (run_id, edar_id)

        Returns:
            Tuple of (flow_xr, censor_xr) with dimensions (run_id, date, edar_id, variant)
            Compatible with EDARProcessor.process_from_xarray()
        """
        print("Extracting EDAR biomarkers...")

        from constants import EDAR_BIOMARKER_PREFIX, EDAR_BIOMARKER_VARIANTS

        flow_list = []
        censor_list = []

        for variant in EDAR_BIOMARKER_VARIANTS:
            var_name = f"{EDAR_BIOMARKER_PREFIX}{variant}"
            lod_name = f"{var_name}_LoD"

            if var_name not in ds:
                continue
            if lod_name not in ds:
                print(f"  Warning: {lod_name} not found, skipping {variant}")
                continue

            # Extract biomarker values: (run_id, date, edar_id)
            biomarker = ds[var_name]
            lod = ds[lod_name]  # (run_id, edar_id,)

            # Filter by date range
            start_date = np.datetime64(self.config.start_date)
            end_date = np.datetime64(self.config.end_date)

            time_mask = (biomarker[TEMPORAL_COORD] >= start_date) & (
                biomarker[TEMPORAL_COORD] <= end_date
            )
            biomarker_filtered = biomarker.isel({TEMPORAL_COORD: time_mask})
            lod_filtered = lod  # LoD doesn't vary by time

            # Compute censor flag: 1.0 if value <= LoD, else 0.0 (for finite values only)
            censor = xr.where(
                (biomarker_filtered <= lod_filtered) & biomarker_filtered.notnull(),
                1.0,
                0.0,
            ).fillna(0.0)

            flow_list.append(biomarker_filtered)
            censor_list.append(censor)

        if not flow_list:
            raise ValueError("No EDAR biomarker data found in synthetic dataset")

        # Stack into (run_id, date, edar_id, variant) format
        flow_xr = xr.concat(flow_list, dim="variant").assign_coords(
            variant=EDAR_BIOMARKER_VARIANTS[: len(flow_list)]
        )
        censor_xr = xr.concat(censor_list, dim="variant").assign_coords(
            variant=EDAR_BIOMARKER_VARIANTS[: len(censor_list)]
        )

        num_runs = len(flow_xr.run_id)
        print(f"  ✓ Extracted {len(flow_list)} biomarker variants with {num_runs} runs")
        return flow_xr, censor_xr

    def _extract_population(self, ds: xr.Dataset) -> xr.DataArray:
        """Extract population data.

        Preserves run_id dimension.

        Expected: (run_id, region_id)
        """
        print("Extracting population...")

        pop = ds["population"]  # (run_id, region_id)

        # Verify region coord matches
        pop = pop.rename({"region_id": REGION_COORD})

        num_runs = len(pop.run_id)
        print(f"  ✓ Extracted population with {num_runs} runs: {pop.shape}")
        return pop

    def _extract_hospitalizations(self, ds: xr.Dataset) -> xr.Dataset | None:
        """Extract hospitalizations data from synthetic bundle.

        Expected shape: (run_id, date, region_id)
        Preserves run_id dimension for curriculum training.

        Returns None if hospitalizations not present in synthetic data.
        """
        if "hospitalizations" not in ds:
            print("  Hospitalizations not found in synthetic data")
            return None

        print("Extracting hospitalizations...")

        # Hospitalizations: (run_id, date, region_id) → preserve run_id dimension
        hosp = ds["hospitalizations"]

        # Verify temporal dimension matches config
        hosp = hosp.rename({TEMPORAL_COORD: TEMPORAL_COORD})

        # Crop to config date range (applies to all runs)
        start_date = np.datetime64(self.config.start_date)
        end_date = np.datetime64(self.config.end_date)

        time_mask = (hosp[TEMPORAL_COORD] >= start_date) & (
            hosp[TEMPORAL_COORD] <= end_date
        )
        hosp_filtered = hosp.isel({TEMPORAL_COORD: time_mask})

        if hosp_filtered.sizes[TEMPORAL_COORD] == 0:
            print("  Warning: No hospitalizations data in temporal range")
            return None

        # Create mask and age channels matching EDAR conventions
        # Mask: 1.0 if data available, 0.0 if missing/NaN
        hosp_mask = xr.where(hosp_filtered.notnull(), 1.0, 0.0)
        hosp_mask = hosp_mask.fillna(0.0)

        # Age: Days since last measurement (synthetic data is complete, so age=1)
        hosp_age = xr.ones_like(hosp_filtered)

        num_runs = len(hosp_filtered.run_id)
        print(
            f"  ✓ Extracted hospitalizations with {num_runs} runs: {hosp_filtered.shape}"
        )

        return xr.Dataset(
            {
                "hospitalizations": hosp_filtered,
                "hospitalizations_mask": hosp_mask,
                "hospitalizations_age": hosp_age,
            }
        )

    def _extract_deaths(self, ds: xr.Dataset) -> xr.Dataset | None:
        """Extract deaths data from synthetic bundle.

        Expected shape: (run_id, date, region_id)
        Preserves run_id dimension for curriculum training.

        Returns None if deaths not present in synthetic data.
        """
        if "deaths" not in ds:
            print("  Deaths not found in synthetic data")
            return None

        print("Extracting deaths...")

        # Deaths: (run_id, date, region_id) → preserve run_id dimension
        deaths = ds["deaths"]

        # Verify temporal dimension matches config
        deaths = deaths.rename({TEMPORAL_COORD: TEMPORAL_COORD})

        # Crop to config date range (applies to all runs)
        start_date = np.datetime64(self.config.start_date)
        end_date = np.datetime64(self.config.end_date)

        time_mask = (deaths[TEMPORAL_COORD] >= start_date) & (
            deaths[TEMPORAL_COORD] <= end_date
        )
        deaths_filtered = deaths.isel({TEMPORAL_COORD: time_mask})

        if deaths_filtered.sizes[TEMPORAL_COORD] == 0:
            print("  Warning: No deaths data in temporal range")
            return None

        # Create mask and age channels matching EDAR conventions
        # Mask: 1.0 if data available, 0.0 if missing/NaN
        deaths_mask = xr.where(deaths_filtered.notnull(), 1.0, 0.0)
        deaths_mask = deaths_mask.fillna(0.0)

        # Age: Days since last measurement (synthetic data is complete, so age=1)
        deaths_age = xr.ones_like(deaths_filtered)

        num_runs = len(deaths_filtered.run_id)
        print(f"  ✓ Extracted deaths with {num_runs} runs: {deaths_filtered.shape}")

        return xr.Dataset(
            {
                "deaths": deaths_filtered,
                "deaths_mask": deaths_mask,
                "deaths_age": deaths_age,
            }
        )

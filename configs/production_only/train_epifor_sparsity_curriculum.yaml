# Sparsity-based curriculum learning configuration

env: "mn5"
# This configuration demonstrates progressive training from low sparsity (clean data)
# to high sparsity (real-world conditions) using synthetic data with varying
# sparsity levels [0.05, 0.2, 0.4, 0.6, 0.8].
#
# The curriculum sampler filters synthetic runs by sparsity range at each phase,
# enabling the model to first learn dynamics from clean data, then progressively
# adapt to noisier, more realistic conditions.

data:
  # Use valid_targets mask from dataset to filter target nodes
  use_valid_targets: false
  # Path to the processed zarr dataset containing both real and synthetic runs
  dataset_path: data/processed/synthetic_full.zarr
  real_dataset_path: data/processed/real_with_id.zarr
  # Mobility-weighted lagged case features (imported risk)
  use_imported_risk: false
  mobility_lags: [1, 7, 14] # Lags in days (only used when use_imported_risk=true)
  region2vec_path: outputs/region_embeddings/region_embeddings.pt
  mobility_threshold: 10
  window_stride: 1
  missing_permit: 2
  sample_ordering: "time"
  log_scale: true
  # run_id is ignored when curriculum is enabled (sampler handles run selection)
  run_id: "0_Baseline"

model:
  type:
    cases: true
    regions: true
    biomarkers: true
    mobility: true
  params:
    mobility_embedding_dim: 16
    region_embedding_dim: 64
    gnn_module: "gcn"
    #-- seq sizes --#
    history_length: 21
    forecast_horizon: 7
    #-- graph params --#
    max_neighbors: 20
    gnn_depth: 2

output:
  experiment_name: sparsity_curriculum
  log_dir: outputs/training
  wandb_mode: offline
  wandb_tags: [mn5]

training:
  device: "cuda"
  batch_size: 32
  gradient_accumulation_steps: 2
  early_stopping_patience: 10
  nan_loss_patience: 5
  # max_batches: 20
  epochs: 20
  learning_rate: 0.0005
  weight_decay: 1.0e-05
  num_workers: 8
  val_workers: 2
  prefetch_factor: 2
  val_split: 0.2
  test_split: 0.1

  # Sparsity-based curriculum configuration
  curriculum:
    enabled: true
    # Number of synthetic runs to sample from per epoch
    # 3 is recommended for diversity while maintaining reasonable memory overhead
    active_runs: 3
    # Contiguous windows per run before rotating to next run
    chunk_size: 512
    # How to select runs within sparsity range: "round_robin" or "random"
    run_sampling: "round_robin"

    schedule:
      # Phase 0: Clean synthetic only (learn dynamics without noise)
      - start_epoch: 0
        end_epoch: 3
        synth_ratio: 1.0 # 100% synthetic data
        mode: "time_major"
        min_sparsity: 0.0 # Captures 0.05, 0.20 (2 runs)
        max_sparsity: 0.25

      # Phase 1: Add light noise + mix with real data
      - start_epoch: 3
        end_epoch: 7
        synth_ratio: 0.8 # 80% synthetic, 20% real
        mode: "time_major"
        min_sparsity: 0.0 # Captures 0.05, 0.20, 0.40 (3 runs)
        max_sparsity: 0.45

      # Phase 2: Medium noise, balance synthetic/real
      - start_epoch: 7
        end_epoch: 12
        synth_ratio: 0.5 # 50% synthetic, 50% real
        mode: "time_major"
        min_sparsity: 0.15 # Captures 0.20, 0.40, 0.60, 0.80 (4 runs)
        max_sparsity: 0.85

      # Phase 3: High noise (approach real-world conditions)
      - start_epoch: 12
        end_epoch: 20
        synth_ratio: 0.2 # 20% synthetic, 80% real
        mode: "time_major"
        min_sparsity: 0.35 # Captures 0.40, 0.60, 0.80 (3 runs)
        max_sparsity: 1.0

  profiler:
    enabled: false
    profile_batches: 10
    log_dir: auto
  loss:
    name: "composite"
    components:
      - name: "smape"
        weight: 1.0
      - name: "mse_unscaled"
        weight: 1.0
  plot_forecasts: true
  num_forecast_samples: 3

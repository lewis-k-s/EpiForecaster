# Production config for training on synthetic data with MN5-scale settings

env: "mn5"
#
# WARNING: DO NOT RUN THIS CONFIG LOCALLY. This config uses MN5-scale settings
# with high memory requirements. Even with reduced workers/batch size, the
# synthetic dataset may crash your machine due to memory constraints (OOM).
#
# This config uses the same hyperparameters and training settings as
# train_epifor_mn5_full.yaml but runs on synthetic data instead of real.
# Useful for development, debugging, and controlled experiments.
#
# Temporal splitting uses all regions as targets but divides data by date ranges.
# Splits are defined as:
# - Train: [start, train_end_date)
# - Val:   [train_end_date, val_end_date)
# - Test:  [val_end_date, test_end_date) or end of dataset if test_end_date is omitted

data:
  dataset_path: data/processed/synthetic_full.zarr
  region2vec_path: checkpoints/region_embedding/region_embeddings.pt
  use_valid_targets: true
  mobility_threshold: 10
  missing_permit: 2
  window_stride: 1
  sample_ordering: "time"
  log_scale: true
  run_id: "0_Baseline" # Synthetic run to use for training
  run_id_chunk_size: 1 # Load one run at a time for memory safety

model:
  type:
    cases: true
    regions: true
    biomarkers: true
    mobility: true
  params:
    mobility_embedding_dim: 32
    region_embedding_dim: 64
    gnn_module: "gcn"
    #-- seq sizes --#
    history_length: 21
    forecast_horizon: 7
    #-- graph params --#
    max_neighbors: 20
    gnn_depth: 2

output:
  experiment_name: mn5_synth_temporal
  log_dir: outputs/training
  wandb_mode: offline
  wandb_tags: [mn5]

training:
  device: "cuda"
  batch_size: 12 # Increased from 8 for better GPU utilization
  gradient_accumulation_steps: 2 # Effective batch = 24
  early_stopping_patience: 5
  nan_loss_patience: 5
  epochs: 10
  learning_rate: 0.0005
  weight_decay: 1.0e-05
  pin_memory: true
  # Worker configuration: treat total workers as shared budget across loaders
  # With 3 loaders (train/val/test), num_workers applies to each unless overridden
  num_workers: 16 # Training workers - increased from 12 after memory analysis
  val_workers: 4 # Validation workers - runs periodically
  test_workers: 0 # Test workers - runs once at end, no prefetching needed
  prefetch_factor: 4 # Batches to prefetch per worker (kept at 4)

  # Temporal split configuration
  # Using temporal split for synthetic data to maximize training samples
  # With 113 days total (2020-02-09 to 2020-06-01), this gives more samples than node split
  split_strategy: "time"
  train_end_date: "2020-04-20"  # ~70% for training
  val_end_date: "2020-05-15"     # ~15% for validation
  # test_end_date omitted - uses all remaining data (2020-05-15 to 2020-06-01)

  # ignored when split_strategy="time"
  # val_split: 0.1
  # test_split: 0.1

  profiler:
    enabled: true
    profile_batches: 20
    log_dir: auto
  plot_forecasts: true
  num_forecast_samples: 3
  # Joint inference loss configuration
  loss:
    name: "joint_inference"
    joint: {}
